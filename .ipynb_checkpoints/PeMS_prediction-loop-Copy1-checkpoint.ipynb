{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7105d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40157055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvのパスを設定\n",
    "csv_path = \"/Users/ishigurofutoshi/lab_materials/data/integrated_route.csv\"\n",
    "#corr_path = \"C:\\\\Users\\\\meida\\\\lab_materials\\\\M1\\\\pems\\\\data\\\\corr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502ca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path,index_col=0)\n",
    "# Slice [start:stop:step], starting from index 5 take every 6th record.\n",
    "#df = df[5::6]\n",
    "df = df.dropna(axis=1)\n",
    "date_time = pd.to_datetime(df.pop('5 Minutes'), format='%Y.%m.%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac576def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#相関係数0.8以上（11個）\n",
    "#df = df.loc[:,['11750', '18828', '18830', '275', '4649', '4651', '6224', '6225', '78','79', '81']]\n",
    "\n",
    "#近くの経路（96個）\n",
    "#df = df.loc[:,['11750', \"4649\",\"79\",\"6225\",\"18830\",\"81\",\"275\",\"78\",\"18828\",\"6224\",\"19835\",\"2132\",\"14467\",\"4648\",\"77\",\"82\",\"80\",\"372\",\"19837\",\"6670\",\"86\",\"6218\",\"6221\",\"271\",\"6679\",\"18862\",\"18846\",\"85\",\"6680\",\"18860\",\"6215\",\"274\",\"13867\",\"18852\",\"276\",\"2135\",\"6220\",\"6202\",\"365\",\"6678\",\"366\",\"413\",\"359\",\"18820\",\"88\",\"379\",\"16207\",\"360\",\"6701\",\"6669\",\"6203\",\"19979\",\"367\",\"6216\",\"6616\",\"6700\",\"6662\",\"6214\",\"272\",\"361\",\"6606\",\"2131\",\"2134\",\"363\",\"18822\",\"87\",\"6665\",\"22622\",\"6200\",\"6666\",\"18850\",\"6217\",\"18832\",\"6671\",\"6604\",\"6605\",\"6219\",\"6663\",\"3394\",\"19981\",\"6201\",\"6601\",\"6675\",\"362\",\"18854\",\"18856\",\"18834\",\"6600\",\"6603\",\"6602\",\"6705\",\"6664\",\"6615\",\"6674\",\"6673\",\"18858\"]]\n",
    "\n",
    "#相関係数0.7以上（77個）\n",
    "#df = df.loc[:,['11750', '13147', '13867', '14467', '16507', '16509', '16510', '16511','18174', '18824', '18828', '18830', '18836', '18838', '18840', '18846','18852', '18860', '18862', '18868', '18872', '19835', '19837', '19839','19955', '20274', '2132', '2133', '2135', '271', '274', '275', '276','358', '365', '366', '369', '372', '373', '412', '425', '434', '4647','4648', '4649', '4650', '4651', '471', '571', '6202', '6210', '6212', '6215', '6218', '6220', '6221', '6222', '6224', '6225', '6226', '6227','6641', '6670', '6677', '6678', '6679', '6680', '6723', '77', '78','79', '80', '81', '82', '83', '85', '86']]\n",
    "\n",
    "#ＤＴＷで11個選ぶ\n",
    "#df = df.loc[:,['11750','4649','79','6225','18830','6224','18828','81','18174','4651','16507']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093e41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 6\n",
    "n_input = 30\n",
    "input_width = 24\n",
    "ran = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea9f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0286a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c57b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cefda6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_test_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46345569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa119f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "                #metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  start = time.time()\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val\n",
    "                      #callbacks=[early_stopping]\n",
    "                     )\n",
    "  run_time = time.time() - start\n",
    "  return history,run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb442a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process1(df, target):\n",
    "    column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    num_features = df.shape[1]\n",
    "    \n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    val_df = (val_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "    \n",
    "    df_std = (df - train_mean) / train_std\n",
    "    df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "    \n",
    "    class WindowGenerator():\n",
    "      def __init__(self, input_width, label_width, shift,\n",
    "                   train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                   label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "          self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "      def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n",
    "                     label_columns=[target])\n",
    "    \n",
    "    WindowGenerator.split_window = split_window\n",
    "    \n",
    "    # Stack three slices, the length of the total window.\n",
    "    example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
    "                               np.array(train_df[100:100+w2.total_window_size]),\n",
    "                               np.array(train_df[200:200+w2.total_window_size])])\n",
    "\n",
    "    example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "    w2.example = example_inputs, example_labels\n",
    "    \n",
    "    WindowGenerator.make_dataset = make_dataset\n",
    "    WindowGenerator.make_test_dataset = make_test_dataset\n",
    "    \n",
    "    WindowGenerator.train = train\n",
    "    WindowGenerator.val = val\n",
    "    WindowGenerator.test = test\n",
    "    WindowGenerator.example = example\n",
    "    \n",
    "    single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=[target])\n",
    "\n",
    "\n",
    "    val_performance = {}\n",
    "    performance = {}\n",
    "    \n",
    "    \n",
    "    wide_window = WindowGenerator(\n",
    "        input_width=input_width, label_width=1, shift=shift,\n",
    "        label_columns=[target])\n",
    "\n",
    "    #wide_window = WindowGenerator(\n",
    "    #input_width=24, label_width=24, shift=1,\n",
    "    #label_columns=[target])\n",
    "    \n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    #tf.keras.layers.LSTM(300, return_sequences=True),\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    history,run_time = compile_and_fit(lstm_model, wide_window)\n",
    "    IPython.display.clear_output()\n",
    "    val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\n",
    "    performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n",
    "    lstm_predict = lstm_model.predict(wide_window.test, verbose=0)\n",
    "    \n",
    "    plt.plot([i for i in range(ran)],test_df[target][input_width+shift-1:ran+input_width+shift-1],color=\"red\")\n",
    "    plt.plot([i for i in range(ran)],np.ravel(lstm_predict[0:ran]),color=\"blue\")\n",
    "    plt.show()\n",
    "    \n",
    "    del df,df_std,test_df,train_df,train_mean,train_std,val_df\n",
    "    \n",
    "    return run_time,val_performance['LSTM'][1],performance['LSTM'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144307c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/Users/ishigurofutoshi/lab_materials/output/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641c2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path,\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"route\",\"runtime\",\"val_performance\",\"performance\",\"input\",\"input_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47777fca",
   "metadata": {},
   "source": [
    "## 相関係数で特徴選択"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c798400",
   "metadata": {},
   "source": [
    "def select_index(target):\n",
    "\n",
    "    #相関係数\n",
    "    corrs = []\n",
    "    for c in df.columns:\n",
    "        corr = np.corrcoef(df[c],df[target])[0,1]\n",
    "        corrs.append(corr)\n",
    "    corrs = np.array(corrs)\n",
    "\n",
    "    #重要度の上位を出力\n",
    "    idx_corrs = np.argsort(np.abs(corrs))[::-1]\n",
    "    top_cols_corrs,top_importances_corrs = df.columns.values[idx_corrs][:n_input],corrs[idx_corrs][:n_input]\n",
    "    print(top_cols_corrs,top_importances_corrs)\n",
    "\n",
    "    top_cols_select = top_cols_corrs\n",
    "    \n",
    "    return top_cols_select\n",
    "    #return [str(target)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536b328",
   "metadata": {},
   "source": [
    "## 相互相関係数で特徴選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94a7abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_index(target):\n",
    "\n",
    "    #相互相関関数\n",
    "    corrs = []\n",
    "    lags = []\n",
    "    for c in df.columns:\n",
    "        xcorr_list = xcorr(df[target],df[c])\n",
    "        lag = np.argmax(np.abs(xcorr_list))\n",
    "        lags.append(lag)\n",
    "        corr = corr_lag(df[target],df[c],lag)\n",
    "        corrs.append(corr)\n",
    "    corrs = np.array(corrs)\n",
    "    lags = np.array(lags) #targetと各routeでのラグを格納\n",
    "    \n",
    "    #重要度の上位を出力\n",
    "    idx_corrs = np.argsort(np.abs(corrs))[::-1]\n",
    "    top_cols_corrs,top_importances_corrs,lags = df.columns.values[idx_corrs][:n_input],corrs[idx_corrs][:n_input],lags[idx_corrs][:n_input]\n",
    "    print(top_cols_corrs,top_importances_corrs,lags)\n",
    "    \n",
    "    #targetのインデックスを追加\n",
    "    if not target in top_cols_corrs:\n",
    "        top_cols_corrs = np.append(top_cols_corrs,target)\n",
    "    \n",
    "    top_cols_select = top_cols_corrs\n",
    "    \n",
    "    return top_cols_select\n",
    "    #return [str(target)]\n",
    "    \n",
    "def xcorr(target,c):\n",
    "    xcorr_list = []\n",
    "    #margin = np.zeros(24)\n",
    "    #target = np.block([target,margin])\n",
    "    #c = np.block([margin,c])\n",
    "    \n",
    "    for t in range(24):\n",
    "        score = np.dot(target[t:],c[:len(c)-t])/(len(target)-t)\n",
    "        xcorr_list.append(score)\n",
    "        \n",
    "    return xcorr_list\n",
    "\n",
    "def corr_lag(target,c,lag):\n",
    "    return np.corrcoef(target[lag:],c[:len(c)-lag])[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cf6b1",
   "metadata": {},
   "source": [
    "## 相関○○以上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb198b1",
   "metadata": {},
   "source": [
    "for target in df.columns:\n",
    "    #corr_index = df.index.values\n",
    "    corr_index = []\n",
    "    index_list = df_corr[df_corr[target]>thre].index.values\n",
    "    for i in index_list:\n",
    "        corr_index.append(str(i))\n",
    "    df_input = df.loc[:,corr_index]\n",
    "    \n",
    "    print(target)\n",
    "    print(df_input.columns)\n",
    "    run_time, val_performance, performance = process1(df_input,target)\n",
    "    print(run_time)\n",
    "    #print(val_performance)\n",
    "    print(performance)\n",
    "    \n",
    "    output = []\n",
    "    output.append(target)\n",
    "    output.append(run_time)\n",
    "    output.append(val_performance)\n",
    "    output.append(performance)\n",
    "    output.append(corr_index)\n",
    "    output.append(len(corr_index))\n",
    "    \n",
    "    with open(output_path,\"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061fffe",
   "metadata": {},
   "source": [
    "## 相関上位○○個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a90e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11750' '4649' '79' '6225' '18830' '81' '78' '4651' '18828' '6224' '275'\n",
      " '16509' '19835' '16507' '2132' '2133' '14467' '4648' '16511' '77' '82'\n",
      " '4650' '16510' '80' '18174' '372' '6222' '19955' '19837' '18824'] [1.         0.9902075  0.96206984 0.840221   0.83876349 0.82030941\n",
      " 0.81900472 0.81898798 0.81583006 0.81549603 0.81518512 0.78766017\n",
      " 0.78353814 0.78197782 0.77670699 0.77623107 0.77569673 0.77349558\n",
      " 0.77338867 0.77262795 0.77144038 0.77135655 0.76728232 0.7647044\n",
      " 0.75817661 0.75786129 0.75765523 0.75765523 0.75596467 0.75455299] [0 0 3 1 1 1 3 0 4 4 0 0 1 0 0 4 0 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
      "11750\n",
      "Index(['11750', '4649', '79', '6225', '18830', '81', '78', '4651', '18828',\n",
      "       '6224', '275', '16509', '19835', '16507', '2132', '2133', '14467',\n",
      "       '4648', '16511', '77', '82', '4650', '16510', '80', '18174', '372',\n",
      "       '6222', '19955', '19837', '18824'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 00:10:18.235156: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method split_window of Total window size: 30\n",
      "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "Label indices: [29]\n",
      "Label column name(s): ['11750']> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method split_window of Total window size: 30\n",
      "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "Label indices: [29]\n",
      "Label column name(s): ['11750']> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f94059b6e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f94059b6e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1750/1752 [============================>.] - ETA: 0s - loss: 0.1239 - root_mean_squared_error: 0.3519WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f93ea9f6050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f93ea9f6050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1752/1752 [==============================] - 38s 20ms/step - loss: 0.1239 - root_mean_squared_error: 0.3520 - val_loss: 0.1952 - val_root_mean_squared_error: 0.4418\n",
      "Epoch 2/20\n",
      " 161/1752 [=>............................] - ETA: 21s - loss: 0.1031 - root_mean_squared_error: 0.3211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ns/m6xqd78d419c4bs1hmdmhkh00000gp/T/ipykernel_83773/4007146884.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrun_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_performance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print(val_performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ns/m6xqd78d419c4bs1hmdmhkh00000gp/T/ipykernel_83773/2520075923.py\u001b[0m in \u001b[0;36mprocess1\u001b[0;34m(df, target)\u001b[0m\n\u001b[1;32m    107\u001b[0m     ])\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mval_performance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwide_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ns/m6xqd78d419c4bs1hmdmhkh00000gp/T/ipykernel_83773/1412197883.py\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, window, patience)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   history = model.fit(window.train, epochs=MAX_EPOCHS,\n\u001b[0;32m---> 14\u001b[0;31m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                       \u001b[0;31m#callbacks=[early_stopping]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                      )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pems/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for target in df.columns:\n",
    "    top_cols_select = select_index(target)\n",
    "    df_input = df[top_cols_select]\n",
    "    \n",
    "    print(target)\n",
    "    print(df_input.columns)\n",
    "    run_time, val_performance, performance = process1(df_input,target)\n",
    "    print(run_time)\n",
    "    #print(val_performance)\n",
    "    print(performance)\n",
    "    \n",
    "    output = []\n",
    "    output.append(target)\n",
    "    output.append(run_time)\n",
    "    output.append(val_performance)\n",
    "    output.append(performance)\n",
    "    output.append(top_cols_select)\n",
    "    output.append(len(top_cols_select))\n",
    "    \n",
    "    with open(output_path,\"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f6d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ebfda3d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6847fd58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
